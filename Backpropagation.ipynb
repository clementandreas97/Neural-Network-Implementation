{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import done.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from numpy.random import seed\n",
    "from random import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import arff\n",
    "\n",
    "print('Import done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1fb0e58f2ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdatafile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weather.arff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# weather = weather.drop(columns=[1,2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Data (weather)\n",
    "\n",
    "datafile = arff.load(open('weather.arff'))\n",
    "weather = pd.DataFrame(datafile['data'])\n",
    "# weather = weather.drop(columns=[1,2])\n",
    "\n",
    "weather[0] = weather[0].map(dict(sunny=0, overcast=1, rainy=2)).astype(float)\n",
    "weather[3] = weather[3].map(dict(TRUE=1, FALSE=0)).astype(float)\n",
    "weather[4] = weather[4].map(dict(yes=1, no=0)).astype(float)\n",
    "\n",
    "x = weather.drop(columns=[4])\n",
    "y = weather[4]\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    outlook  temp  humidity  windy\n",
      "0       0.0   0.0       1.0    0.0\n",
      "1       0.0   0.0       1.0    1.0\n",
      "2       1.0   0.0       1.0    0.0\n",
      "3       2.0   1.0       1.0    0.0\n",
      "4       2.0   2.0       0.0    0.0\n",
      "5       2.0   2.0       0.0    1.0\n",
      "6       1.0   2.0       0.0    1.0\n",
      "7       0.0   1.0       1.0    0.0\n",
      "8       0.0   2.0       0.0    0.0\n",
      "9       2.0   1.0       0.0    0.0\n",
      "10      0.0   1.0       0.0    1.0\n",
      "11      1.0   1.0       1.0    1.0\n",
      "12      1.0   0.0       0.0    0.0\n",
      "13      2.0   1.0       1.0    1.0\n",
      "\n",
      "[0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Data (tennis)\n",
    "\n",
    "weather = pd.read_csv('tennis.csv')\n",
    "\n",
    "weather.outlook = weather.outlook.map(dict(sunny=0, overcast=1, rainy=2)).astype(float)\n",
    "weather.temp = weather.temp.map(dict(hot=0, mild=1, cool=2)).astype(float)\n",
    "weather.humidity = weather.humidity.map(dict(normal=0, high=1)).astype(float)\n",
    "weather.windy = weather.windy.astype(float)\n",
    "weather.play = weather.play.map(dict(yes=1, no=0)).astype(float)\n",
    "\n",
    "x = weather.drop(columns=['play'])\n",
    "y = weather.play\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [2. 1. 1. 1.]\n",
      " [2. 1. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [2. 1. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [2. 2. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "y_train\n",
      "[1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1.]\n",
      "\n",
      "X_validation\n",
      "[[0. 2. 0. 0.]]\n",
      "\n",
      "y_validation\n",
      "[1.]\n",
      "\n",
      "X_testing\n",
      "[[2. 2. 0. 0.]\n",
      " [1. 2. 0. 1.]]\n",
      "\n",
      "y_testing\n",
      "[1. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "X_validation, X_testing, y_validation, y_testing = train_test_split(X_test,\n",
    "                                                                   y_test,\n",
    "                                                                   test_size=0.5,\n",
    "                                                                   random_state=0)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_validation = np.asarray(X_validation)\n",
    "X_testing = np.asarray(X_testing)\n",
    "y_train = np.asarray(y_train)\n",
    "y_validation = np.asarray(y_validation)\n",
    "y_testing = np.asarray(y_testing)\n",
    "\n",
    "print('X_train')\n",
    "print(X_train)\n",
    "print()\n",
    "\n",
    "print('y_train')\n",
    "print(y_train)\n",
    "print()\n",
    "\n",
    "print('X_validation')\n",
    "print(X_validation)\n",
    "print()\n",
    "\n",
    "print('y_validation')\n",
    "print(y_validation)\n",
    "print()\n",
    "\n",
    "print('X_testing')\n",
    "print(X_testing)\n",
    "print()\n",
    "\n",
    "print('y_testing')\n",
    "print(y_testing)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation class\n",
    "\n",
    "class BackPropagation:\n",
    "    n_inputs = 1\n",
    "    n_hiddens = [1]\n",
    "    n_outputs = 1\n",
    "    l_rate = 0.5\n",
    "    n_epoch = 20\n",
    "    batch_size = 1\n",
    "    momentum = 1\n",
    "    network = list()\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hiddens, l_rate, n_epoch, batch_size, momentum):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.l_rate = l_rate\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.momentum = momentum\n",
    "        self.network = list()\n",
    "\n",
    "        hidden_layer = [{'weights':[random() for i in range(self.n_inputs + 1)]} for i in range(self.n_hiddens[0])]\n",
    "        self.network.append(hidden_layer)\n",
    "\n",
    "        for i in range(1, min(len(n_hiddens), 10)):\n",
    "            hidden_layer = [{'weights':[random() for j in range(self.n_hiddens[i-1] + 1)]} for i in range(self.n_hiddens[i])]\n",
    "            self.network.append(hidden_layer)\n",
    "\n",
    "        output_layer = [{'weights':[random() for i in range(self.n_hiddens[-1] + 1)]} for i in range(self.n_outputs)]\n",
    "        self.network.append(output_layer)\n",
    "        \n",
    "    def activate(self, weights, inputs):\n",
    "        activation = weights[-1]\n",
    "        for i in range(len(weights)-1):\n",
    "            activation += weights[i] * inputs[i]\n",
    "        return activation\n",
    "\n",
    "    def transfer(self, activation):\n",
    "        return 1.0 / (1.0 + np.exp(-activation))\n",
    "\n",
    "    def forward_propagate(self, network, init_input):\n",
    "        inputs = init_input\n",
    "        for layer in network:\n",
    "            new_inputs = []\n",
    "            for neuron in layer:\n",
    "                activation = self.activate(neuron['weights'], inputs)\n",
    "                neuron['output'] = self.transfer(activation)\n",
    "                new_inputs.append(neuron['output'])\n",
    "            inputs = new_inputs\n",
    "        return inputs\n",
    "    \n",
    "    def transfer_derivative(self, output):\n",
    "        return output * (1.0 - output)\n",
    "\n",
    "    def backward_propagate_error(self, network, expected):\n",
    "        for i in reversed(range(len(network))):\n",
    "            layer = network[i]\n",
    "            errors = list()\n",
    "            if i != len(network)-1:\n",
    "                for j in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in network[i + 1]:\n",
    "                        error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron = layer[j]\n",
    "                    a = 1\n",
    "                    errors.append(expected[j] - neuron['output'])\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                neuron['delta'] = errors[j] * self.transfer_derivative(neuron['output'])\n",
    "    \n",
    "    def initialize_delta_weight(self, network):\n",
    "        delta_weight = []\n",
    "        for layer in network:\n",
    "            delta_weight.append([])\n",
    "            for neuron in layer:\n",
    "                delta_weight[-1].append(np.zeros(len(neuron['weights'])))\n",
    "        return delta_weight\n",
    "\n",
    "    def accumulate_delta_weight(self, network, init_input, l_rate, momentum, delta_weight):\n",
    "        for i in range(len(network)):\n",
    "            inputs = init_input[:-1]\n",
    "            if i != 0:\n",
    "                inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "            for j in range(len(network[i])):\n",
    "                for k in range(len(inputs)):\n",
    "                    delta_weight[i][j][k] += (l_rate * network[i][j]['delta'] * inputs[k] + momentum * network[i][j]['weights'][k]) \n",
    "                delta_weight[i][j][-1] += (l_rate * network[i][j]['delta'] + momentum * network[i][j]['weights'][-1]) \n",
    "\n",
    "    def update_weight(self, network, delta_weight):\n",
    "        for i in range(len(network)):\n",
    "            for j in range(len(network[i])):\n",
    "                for k in range(len(network[i][j]['weights'])):\n",
    "                    network[i][j]['weights'][k] += delta_weight[i][j][k]\n",
    "\n",
    "    def fit(self, init_inputs_x, init_inputs_y, validation_data = None):\n",
    "        init_inputs = list()\n",
    "        for i in range(0,len(init_inputs_x)):\n",
    "            init_inputs.append(np.append(init_inputs_x[i],init_inputs_y[i]))\n",
    "        for epoch in range(self.n_epoch):\n",
    "            sum_error = 0\n",
    "            batch_member_calculated = 0\n",
    "            delta_weight = self.initialize_delta_weight(self.network)\n",
    "            for init_data in init_inputs:\n",
    "                outputs = self.forward_propagate(self.network, init_data)\n",
    "                expected = [0 for i in range(self.n_outputs)]\n",
    "                expected[0] = init_data[-1]\n",
    "                sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "                self.backward_propagate_error(self.network, expected)\n",
    "\n",
    "                self.accumulate_delta_weight(self.network, init_data, self.l_rate, self.momentum, delta_weight)\n",
    "                batch_member_calculated += 1\n",
    "\n",
    "                if batch_member_calculated == self.batch_size:\n",
    "                    self.update_weight(self.network, delta_weight)\n",
    "                    batch_member_calculated = 0\n",
    "                    delta_weight = self.initialize_delta_weight(self.network)\n",
    "            if validation_data != None:\n",
    "                pred = self.predict(validation_data[0])\n",
    "                val_score = accuracy_score(pred, validation_data[1])\n",
    "                print('>epoch=%d, lrate=%.3f, error=%.3f, val_score=%.2f, outputs=%.2f' % (epoch, self.l_rate, sum_error, val_score, outputs[0]))\n",
    "            else:\n",
    "                print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, self.l_rate, sum_error))\n",
    "        tempnetwork = list()\n",
    "        for layer in self.network:\n",
    "            templayer = list()\n",
    "            for neuron in layer:\n",
    "                templayer.append({'weights' : neuron['weights']})\n",
    "            tempnetwork.append(templayer)\n",
    "        self.network = tempnetwork\n",
    "            \n",
    "    def predict(self, init_inputs):\n",
    "        labels = np.zeros(len(init_inputs))\n",
    "        for i in range(0,len(init_inputs)):\n",
    "            labels[i] = self.forward_propagate(self.network, init_inputs[i])[0].round()\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=3.555, val_score=1.00, outputs=0.70\n",
      ">epoch=1, lrate=0.500, error=3.201, val_score=1.00, outputs=0.61\n",
      ">epoch=2, lrate=0.500, error=3.028, val_score=1.00, outputs=0.54\n",
      ">epoch=3, lrate=0.500, error=2.986, val_score=1.00, outputs=0.51\n",
      ">epoch=4, lrate=0.500, error=2.982, val_score=1.00, outputs=0.49\n",
      ">epoch=5, lrate=0.500, error=2.983, val_score=1.00, outputs=0.49\n",
      ">epoch=6, lrate=0.500, error=2.984, val_score=1.00, outputs=0.48\n",
      ">epoch=7, lrate=0.500, error=2.984, val_score=1.00, outputs=0.48\n",
      ">epoch=8, lrate=0.500, error=2.983, val_score=1.00, outputs=0.48\n",
      ">epoch=9, lrate=0.500, error=2.982, val_score=1.00, outputs=0.48\n",
      ">epoch=10, lrate=0.500, error=2.981, val_score=1.00, outputs=0.48\n",
      ">epoch=11, lrate=0.500, error=2.980, val_score=1.00, outputs=0.48\n",
      ">epoch=12, lrate=0.500, error=2.979, val_score=1.00, outputs=0.48\n",
      ">epoch=13, lrate=0.500, error=2.978, val_score=1.00, outputs=0.48\n",
      ">epoch=14, lrate=0.500, error=2.978, val_score=1.00, outputs=0.48\n",
      ">epoch=15, lrate=0.500, error=2.977, val_score=1.00, outputs=0.48\n",
      ">epoch=16, lrate=0.500, error=2.977, val_score=1.00, outputs=0.48\n",
      ">epoch=17, lrate=0.500, error=2.976, val_score=1.00, outputs=0.49\n",
      ">epoch=18, lrate=0.500, error=2.976, val_score=1.00, outputs=0.49\n",
      ">epoch=19, lrate=0.500, error=2.975, val_score=1.00, outputs=0.49\n",
      "{'weights': [0.8858639971430188, 0.6537254030337719, 0.7189213347940806, 0.6214709192207821, 0.9102402950134343]}\n",
      "{'weights': [0.5881651041517405, 0.8457290399163963, 0.9747152724330139, 1.1047954110048208, 0.577069629653848]}\n",
      "{'weights': [-0.3668430396466714, 0.3331198621024041, 0.15248666602062094]}\n",
      "\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation training\n",
    "\n",
    "seed(1)\n",
    "tp = (X_validation, y_validation)\n",
    "bp = BackPropagation(n_inputs=4, n_hiddens=[2], l_rate=0.5, n_epoch=20, batch_size=1, momentum=0.001)\n",
    "bp.fit(X_train, y_train, tp)\n",
    "for layer in bp.network:\n",
    "    for neuron in layer:\n",
    "        print(neuron)\n",
    "print()\n",
    "print(bp.predict(X_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 1 samples\n",
      "Epoch 1/20\n",
      "11/11 [==============================] - 1s 54ms/step - loss: 0.7862 - acc: 0.4545 - val_loss: 1.4747 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.9752 - acc: 0.2727 - val_loss: 0.6106 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9164 - acc: 0.1818 - val_loss: 0.4634 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8576 - acc: 0.5455 - val_loss: 1.3133 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8662 - acc: 0.4545 - val_loss: 0.3870 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8612 - acc: 0.3636 - val_loss: 0.9877 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.8498 - acc: 0.3636 - val_loss: 0.4580 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.8560 - acc: 0.4545 - val_loss: 0.5599 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7458 - acc: 0.5455 - val_loss: 0.2240 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.8999 - acc: 0.3636 - val_loss: 0.4585 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8213 - acc: 0.3636 - val_loss: 0.4470 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.7412 - acc: 0.6364 - val_loss: 0.2711 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7481 - acc: 0.6364 - val_loss: 0.3758 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7409 - acc: 0.4545 - val_loss: 0.7465 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7361 - acc: 0.4545 - val_loss: 0.4893 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6640 - acc: 0.4545 - val_loss: 0.3516 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7244 - acc: 0.5455 - val_loss: 0.7994 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6565 - acc: 0.6364 - val_loss: 0.7119 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5844 - acc: 0.6364 - val_loss: 1.2183 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.5646 - acc: 0.8182 - val_loss: 0.8256 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6421593 ],\n",
       "       [0.48254406]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras training\n",
    "\n",
    "seed(1)\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=4, activation=\"sigmoid\", bias_initializer=\"ones\"))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.7, momentum=0)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=20, validation_data=(X_validation, y_validation))\n",
    "model.predict(X_testing, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
